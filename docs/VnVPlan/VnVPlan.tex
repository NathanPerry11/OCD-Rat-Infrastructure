\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct 27 2025 & 1.0 & Initial Document\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents
\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{l l}
  \caption{List of Symbols, Abbreviations, and Acronyms} \\
  \toprule
  \textbf{Symbol / Acronym} & \textbf{Description} \\
  \midrule
  \endfirsthead

  \toprule
  \textbf{Symbol / Acronym} & \textbf{Description (continued)} \\
  \midrule
  \endhead

  \bottomrule
  \multicolumn{2}{r}{\textit{Continued on next page}} \\
  \endfoot

  \bottomrule
  \endlastfoot

  T & Test \\
  SRS & System Requirements Specification \\
  V\&V & Verification and Validation \\
  FR & Functional Requirement \\
  NFR & Nonfunctional Requirement \\
  MIS & Module Interface Specification \\
  MG & Module Guide \\
  CI & Continuous Integration \\
  UI & User Interface \\
  FRDR & Federated Research Data Repository \\
  API & Application Programming Interface \\
  PR & Pull Request \\
  ID & Identification \\
  NLP & Natural Language Processing \\
  SQL & Structured Query Language \\
  CSV & Comma-Separated Values \\
  GTmetrix & Grand Touring Metrix (web performance analysis tool) \\
  APP & Application \\
  EOU & Ease of Use \\
  LEA & Learning \\
  UAP & Understandability and Politeness \\
  ACC & Accessibility \\
  ARIA & Accessible Rich Internet Applications \\
  SAL & Speed and Latency \\
  ROFT & Robustness or Fault Tolerance \\
  CAP & Capacity \\
  SOE & Scalability or Extensibility \\
  MAI & Maintainability \\
  SUP & Supportability \\
  INT & Integrity \\
  IMM & Immunity \\
  OWASP ZAP & Open Web Application Security Project Zed Attack Proxy \\
  LEG & Legal \\
  STA & Standards Compliance \\
  ST-FR & System Test – Functional Requirement \\
  ST-NFR & System Test – Nonfunctional Requirement \\
  LON & Longevity \\
  EPE & Expected Physical Environment \\
  WE & Wider Environment \\
  IWAS & Interfacing with Adjacent Systems \\
  PROD & Productization \\
  REI & Release \\
  ADA & Adaptability \\
\end{longtable}


\newpage

\pagenumbering{arabic}
This section defines the symbols, abbreviations, and acronyms used throughout the plan. The purpose of this section is to
 make sure clarity and 
consistency remains when referring the plan.

\section{General Information}

\subsection{Summary}

The Gouda Engineers team built this entire software system. Their core aim? To deliver a platform—one that's both
 robust and lightning-quick—that doesn't just meet but exceeds every single functional and non-functional requirement laid 
 out in the official SRS.

This document looks into the system's VnV process. This process 
is  essential as it guarantees the software  behaves exactly how it's supposed to and, crucially, completely
 satisfies what the stakeholders want. We will be checking that every single part of the development 
 puzzle—the initial SRS, the intricate design blueprints, the lines of code itself—perfectly aligns with its own set of 
 specifications and validation, is the final check.

Our rigorous VnV approach pulls together a variety of methods: automated testing provides speed, while painstaking manual
 inspections and collaborative peer reviews inject human judgment to evaluate the system's correctness, performance, and 
 general usability. These activities are what fundamentally build confidence in the software's ultimate quality and 
 unwavering reliability, simultaneously rooting out any lingering risks or pesky issues long before the final, official 
 release.


\subsection{Objectives}


   The main objective of this plan is to build confidence in the accuracy, reliability and stability of the software we 
   are developing. The system works as a connection between researchers and the data set of rat trials. Our focus is on providing 
   a program that retrieves correct data and works smoothy with no errors. AN objective is that the layout and information presented
   is usable and doesn't provide any friction in it use. However, we are not able to do user testing on a wide population and are 
   beyond the scope of the project. WE assume that the underlying database and external libraries are already validated.

The plan wants to ensure that the core functions of the interface like data access, filtering and display are
 accurate and reliable,  long-term performance testing is beyond the scope of 
 this phase.

\subsection{Challenge Level and Extras}

The project would be advanced because it uses many complex software engineering ideas and principles. Creating a visually appealing UI, integrating NLP, implementing the store front display while
also ensuring that the performance is good. These features require to make both the desing and verification of the system very in depth to fit the project.\\

The project includes two extras, a performance report and a user manual. The performance report identifies ares of the code where the software can be optimized to make it faster and more effecient, and the user manual
offers the user guidance through the program.


\subsection{Relevant Documentation}

\begin{itemize}
    \item Perry, Nathan, Aidan Goodyer, Jeremy Orr, et al. "OCD-Rat-Infrastructure/Docs/Problemstatementandgoals at Main - OCD-Rats-Capstone/OCD-Rat-Infrastructure." GitHub. Accessed 27 Oct. 2025. \url{https://github.com/OCD-Rats-Capstone/OCD-Rat-Infrastructure/tree/main/docs/ProblemStatementAndGoals}

    \item Perry, Nathan, Aidan Goodyer, Tim Pokanai, et al. "OCD-Rat-Infrastructure/Docs/SRS-Volere at Main - OCD-Rats-Capstone/OCD-Rat-Infrastructure." GitHub. Accessed 27 Oct. 2025. \url{https://github.com/OCD-Rats-Capstone/OCD-Rat-Infrastructure/tree/main/docs/SRS-Volere}

    \item Perry, Nathan, Leo Vugert, Tim Pokanai, Jeremy Orr, et al. "OCD-Rat-Infrastructure/Docs/Hazardanalysis at Main - OCD-Rats-Capstone/OCD-Rat-Infrastructure." GitHub. Accessed 27 Oct. 2025. \url{https://github.com/OCD-Rats-Capstone/OCD-Rat-Infrastructure/tree/main/docs/HazardAnalysis}

    \item Perry, Nathan, Tim Pokanai, Aidan Goodyer, Jeremy Orr, et al. "OCD-Rat-Infrastructure/Docs/Design at Main - OCD-Rats-Capstone/OCD-Rat-Infrastructure." GitHub. Accessed 27 Oct. 2025. \url{https://github.com/OCD-Rats-Capstone/OCD-Rat-Infrastructure/tree/main/docs/Design}

    \item Perry, Nathan, Tim Pokanai, Jeremy Orr, Aidan Goodyer, et al. "OCD-Rat-Infrastructure/Docs/Developmentplan at Main - OCD-Rats-Capstone/OCD-Rat-Infrastructure." GitHub. Accessed 27 Oct. 2025. \url{https://github.com/OCD-Rats-Capstone/OCD-Rat-Infrastructure/tree/main/docs/DevelopmentPlan}
\end{itemize}

\citet{SRS-Volere}

\section{Plan}

\par{The following section will outline a verification and validation plan to follow throughout the project timeline. 
Since our development of the project is executed in multiple sequential steps, this section will outline the verification and validation 
plans, as well as the tools and methods used for each milestone component leading up to our final software solution. This section will also 
list the entities that will participate in the verification and validation steps. First we will outline who will be involved in the 
verification and validation team. Then we will discuss the verification plans of the SRS, design, VnV Plan, implementation, and 
the automatic tools we use for testing and verification. Lastly, we will discuss how we will perform the validation of our software.}


\subsection{Verification and Validation Team}

\begin{itemize}
  \item{\textbf{Aidan Goodyer},\textbf{Jeremy Orr},\textbf{Nathan Perry},\textbf{Timothy Pokanai},\textbf{Leo Vugert}:}
   \par{The five members of the capstone team will split the testing evenly among themselves. All members will be equally expected to
  contribute to the synthesis of both unit and system tests as well as setting up automated tests down the line. Since the author of the System Testing plan was \textbf{Jeremy Orr}
  and the author of the Unit Testing plan will be \textbf{Nathan Perry}, they will be considered the head of system testing and unit testing respectively. Although
  this does not require them to necessarily produce more tests, they will be expected to be prepared to provide guidance to other group members if they need it.}
  \item{\textbf{Dr. Henry Szechtman}, \textbf{Dr. Anna Dvorkin-Gheva}:} 
  \par{ The two supervisors of this project will be involved in the testing process as if they were early
  access users. Given that they represent a demographic very similar to what our actual users will look like (Albeit with a much better understanding of the data),
  their feedback will be extremely useful in understanding how well our system will be recieved by users. They will perform both User Acceptance Testing and
  and Usability testing on our system; trying out the various functionalities and commenting on if they think it is satisfactory and understandable for the
  general user base.}

\end{itemize}

\subsection{SRS Verification}

\subsubsection{SRS Verification Approaches}

\begin{itemize}

  \item{\textbf{Peer Reviewers: } Our first approach for SRS verification is reviewing the peer review issues created by our classmates. This
  is not the most rigorous or systematic but allows us to evaluate feedback on our SRS and based on these comments, determine if the SRS is consistent,
  correct, feasible and complete or if changes are needed.}
  \item{\textbf{ Unstructured Supervisor SRS Review: } Our second approach for SRS verification is to provide our SRS document to our supervisor, Dr. Henry Szechtman and
  allow him to make comments on it. This approach is also quite unstructured but is still useful in our verification process. By giving our supervisor
  the time to look over the SRS, it will elicit any large gaps between our understanding of his goals for the system and our interpretation of these goals
  during our meetings to elicit said requirements. In other words, this will show us any glaring issues in our requirements from the perspective of our
  supervisor.}
  \item{\textbf{Structured Supervisor SRS Review: } Our third approach for SRS verification also involves our supervisor but in a much more structured manner.
  Our approach to this review would be a meeting in which we present to him the key aspects of the SRS and ask him if he sees any issues with these key sections.
  The main sections (note these do not necessarily refer to specific sections in the SRS template itself) to be covered, in descending order of priority are: Functional Requirements and Use Cases, Project Constraints and Scope, Non-Functional Requirements, Project
  Drivers and Project Issues. During this review we would ask him things like: Is this section complete? Is there anything missing? Does the way we laid out
  these requirements seem correct to you? Do these requirements seem feasible to you? Is this in accordance with your conception of what the system will look like?}
  \item{\textbf{Structured Team SRS Review: } Our final approach for SRS verification will be a review in a much similar format to the structured supervisor
  review but only with the five members of our team. We would go over the same key aspects of the SRS and ask mostly the same questions regarding these sections
  as we would in our supervisor review. Doing an internal review like this will be helpful as although the SRS was generated collectively, the actual writing
  of the requirements was done in a very individual and compartmentalized way and this gives each member an opportunity to closely examine the key parts of our
  requirements and provide and criticisms, issues or additions that they feel are necessary to each section.}
\end{itemize}

\subsubsection{SRS Verification Checklist}

\par{Below is a general checklist of questions that our SRS should satisfy (i.e. the answer should be yes):}

\begin{itemize}

  \item{Are all of the functional requirements necessary, feasible, complete and provide unique value to the system?}
  \item{Do the functional requirements cover every expected function of the system?}
  \item{Are all of the use cases necessary, feasible and complete?}
  \item{Do the existing use cases cover every expected function of the system?}
  \item{Does the SRS cover every relevant project constraint such that there is no unaccounted for aspect of the environment that
  may prevent us from implementing some of the system's functionality?}
  \item{Are all project constraints accurately described such that it is clear what exactly is constrained and it is understandable how
  this may affect the system?}
  \item{Is the scope properly defined such that all project components have been accounted for in the scope?}
  \item{Have all relevant out of scope components been listed and rationale provided for why they are out of scope?}
  \item{Are all non-functional requirements necessary, feasible, complete and provide unique value to the system?}
  \item{For non-functional requirements that are not completely necessary, is their value large enough to justify their existence?}
  \item{Have all obligatory non-functional requirements been accounted for, specifically those relating to legality, security or compliance that
        are imposed by entities outside of the system?}

\end{itemize}

\subsection{Design Verification}

\par{For the software design process, the goal of verification of this stage is to ensure our design correctly and completely satisfies 
the SRS before we start coding. The verification will ensure that during the design process we were not focused on 
"building the right thing", but "building the thing right". The following subsections will describe the verification techniques and 
checklist to be used during design verification.}

\subsubsection{Design Verification Techniques}

\begin{itemize}
  \item{\textbf{Structured Supervisor Design Walkthrough:} The first design verification technique will be our team leading a structured 
  walkthrough of our design documents to our supervisors, Dr. Henry Szechtman and Dr. Anna Dvorkin-Gheva. This will allow our team to present 
  our relevant technical design documents in a clear and logically structured manner to provide transparency and room for suggestions 
  on our design decisions, as well as communicate technical aspects as clearly as possible to reviewers with less technical knowledge.
  }
  \item{\textbf{Peer Review from Classmates:} The second design verification technique will be the peer reviews we receive from our classmates. 
  Again, this technique is not expected to be the most rigorous, but another team will review our design process and contribute feedback 
  which provides us an opportunity to verify our design process to make sure it is consistent with our SRS.
  }
  \item{\textbf{Semi-Structured Team Design Inspection:} Our third technique we will use for verifying our design process will be 
  our team doing a semi-structured inspection of our design documents. Since our team is doing all the design and development 
  as the only software engineers involved in this project, we only need to keep this inspection among us as the development team. 
  Doing a semi-structured inspection together would entail going through our relevant technical design documents in a logically 
  structured manner which would ensure our design process aligns with the SRS and there are no ambiguities in the process. Additionally, 
  the semi-structured format would allow room for open-ended discussion on design decisions that could be added, modified, or improved.
  }
  \item{\textbf{Low Fidelity UI Prototype Verification:} Finally our last design verification technique will be creating a low fidelity prototype specifically 
  for the UI component using a tool like Figma or actually coding an interface, allowing our team and our supervisors to test the prototype. 
  A big part of our design process is designing a more user-friendly and accessible UI than the existing one FRDR offers. Conducting a 
  prototype test with our entire verification and validation team is critical, considering we would have the input and suggestions of our 
  two supervisors who interface with FRDR on a regular basis.
  }
\end{itemize}

\subsubsection{Design Verification Checklist}

\par{Below is a general checklist of questions that our Design Process should satisfy (i.e. the answer should be yes):}

\begin{itemize}
  \item{Does the design fulfill all functional requirements stated in the SRS?}
  \item{Does the design adhere to all non-functional requirements devised in the SRS?}
  \item{Is the design feasible to implement considering the given timeline, resources, and constraints of the project?}
  \item{Are all subsystems, modules, and components well-defined and logically decomposed within our overall system?}
  \item{Are the APIs and interfaces between our components clearly specified with their inputs, outputs, and data types?}
  \item{Is there traceability from our system components and architecture all the way back to our requirements?}
  \item{Do all diagrams reflect the architecture of our design consistently and accurately?}
  \item{Do the UIs follow usability and accessibility guidelines?}
  \item{Has the verification and validation team approved our design process?}
\end{itemize}

\subsection{Verification and Validation Plan Verification}

\subsubsection{Verification and Validation Plan Verification Approaches}

\begin{itemize}

  \item{\textbf{Peer Reviewers: } Our first approach for our Verification and Validation Plan verification is reviewing the peer review issues created by our classmates. As
  mentioned in the earlier section, this is not the most rigorous or systematic but allows our peers to provide insight from an outside perspective and gives us the
  chance to evaluate these criticisms and determine if changes need to be made.}
  \item{\textbf{Structured Internal Team VnV Review: } Our second approach for Verification and Validation Plan Verification is to do a structured team review, identical
  to that mentioned in the SRS verification section. As a team, we will go over the checklist for the VnV plan and each member will be given the chance to critique
  the section, identify things that should be changed and personally evaluate sections they may have not written or had the chance to inspect closely.}
  \item{\textbf{Mutation Testing: } The third approach is the only strictly novel approach being used for verifying the Verification and Validation Plan.
  Mutation tests will be used to validate the unit tests once they are developed and as many system testing modules as are applicable (mutation testing
  may not be appropriate for some mutation). Each test that is subject to our mutation testing should satisfy the requirements denoted in the checklist written in
  the next subsection.}
\end{itemize}

\subsubsection{Verification and Validation Plan Verification Checklist}

\textbf{Mutation Testing Specific Checks:}

\begin{itemize}
  \item{Do all relevant test modules have at least one mutation test associated with them?}
  \item{For all test modules, are all mutants caught by the test suite for that module?}
  \item{In the case that a mutant is not caught, can a convincing reason be developed for why this is okay? If not, new tests must be created.}
\end{itemize}

\textbf{General VnV Checklist:}

\begin{itemize}
  \item{Are all the generated tests reasonably valuable and worth the time to implement?}
  \item{Are all the generated tests feasible to implement?}
  \item{Do the tests cover the main anticipated error points that the team can come up with for each module?}
  \item{Are there any glaring and potentially likely failure cases not covered by the test?}
  \item{Does each component of the system contain a test suite to an acceptable level of granularity?}
  \item{Does each test have reasonable and easily justifiable connection to a specific requirement(s)?}
  \item{Does the scope of our testing include all necessary components and are all out of scope components clearly
  justifiable? Can any team member identify a component that may need to be moved from in scope to out of scope or vice versa?}
  \item{Do our tests cover every applicable requirement from our SRS? Are there any requirements which can be reasonably tested
  within the scope of the VnV that are not being tested against?}
\end{itemize}

\subsection{Implementation Verification}


The implementation verification will be used to guide the system's adherance to the specifications as defined in the SRS. Verification will be done using both static and dynamic methods. The team will employ several static tools including linters and code quality analyzers to enforce a high standard and detect vulnerabilities. 





One major source of verification will be our unit test suite, which will be defined for both frontend and backend components of the system and integrated into the CI process. Test cases will target functional requirements as defined in the 


\textbf{Unit Testing:}

Unit Test cases will target functional requirements as defined in the System Tests (see \hyperref[sec:system_tests]{Section~\ref{sec:system_tests}}). These tests will be important drivers of system correctness, ensuring functionality works according to specification. Correctness of querying behaviour (\textbf{FR2}) is one example of a critical requirement to verify. 




\textbf{Performance Testing:}

Performance testing will be used to validate several important benchmarks impacting usability including:
\begin{itemize}
  \item Query Speed (\textbf{NFR8})
  \item  Fault Tolerance (\textbf{NFR9}),
  \item Scalability (\textbf{NFR10}). 
\end{itemize}


Performance testing will utilize end-to-end methods to validate metrics like query speed, where the response time can be measured using an automated UI test. 

\textbf{Static Analysis:}

Static analysis tools will be employed to maintain a consistent quality threshold throughout the development lifecycle, helping adhere to style guidelines and protecting against defects early on. Specific tools are defined in section 3.6.  

\textbf{Code Walkthroughs and Inspections}

\begin{itemize}
  \item \textbf{Peer Review:} Pull Requests will go through a mandatory peer review process prior to any PR being merged. A minimum of 2 reviewers will be required prior to merging.  
\end{itemize}

\textbf{Manual Reviews:}

To verify many of the non-functional requirements, the team will employ manual reviews and user studies to ensure the sytem meets look, feel, and usability requirements.  Walkthroughs will be useful for several of the following: 

\begin{itemize}
    \item NFR3 UI Familiarity
  \item NFR4 Ease of Use
 \item NFR5 Learning Curve Assessment
\end{itemize}


\subsection{Automated Testing and Verification Tools}

The following tools are to be used during the verification and validation process to facilitate automated testing and verification. Broadly, these tools are separated into frontend and backend categories:

\textbf{Frontend:}
\begin{itemize}
  \item Jest: Facilitates frontend unit testing and coverage 
  \item ESLint: A JavaScript/TypeScript linter with support for JSX
  \item Puppeteer: End to end testing and automated UI testing.
\end{itemize}

\textbf{Backend:}
\begin{itemize}
  \item flake8: Style guide enforcement for Python backend
  \item PyTest: Unit testing and integration testing. 
\end{itemize}

\textbf{Continuous Integration:}

We will use GitHub actions for CI purposes, which will be configured to run the complete test suite for the frontend and backend upon commit, and serve as a quality gate prior to PR merges. Jobs will also be configured to run static analysis and linting checks, as well as generate coverage reports. 


\subsection{Software Validation}


This section outlines our plan for validating the system to ensure it meets the needs of our stakeholders and users. Our core goal is to confirm the system provides an accessible and non-technical interface for researchers to analyze the rat behavioural data. The project is not defined in terms of specific performance metrics, but instead by its utility as an exploratory platform for researchers. Therefore, our validation strategy focuses on researcher utility and data integrity. 


The following methods will be used to engage our project stakeholders and elicit feedback throughout the project lifecycle. 

\begin{itemize}
  \item \textbf{ Revision 0 Demonstration:}  The revision 0 demonstratation will be an important validation milestone for our software requirements. We will present this revision of the system to our supervisors to demonstrate the core features of the system. This will allow us to validate that the implementation aligns with the client goals and needs effectively. 
  \item \textbf{Iterative Supervisor Feedback:} The team will leverage our weekly scheduled meetings with Dr. Szechtman and Dr. Dvorkin-Gheva to elicit ongoing feedback during the project's development. These meetings will provide continuous validation and will help us ensure that the project is aligned with our stakeholder expectations.  
  \item \textbf{User Testing:} Towards the end of the developnent cycle we will conduct user testing. The user will be asked to perform tasks like executing a natural language query, filtering the data, or producing a visualization. We will use this user feedback to validate that the system is ready for its intended users.  
 \end{itemize}

\label{sec:system_tests}
\section{System Tests}

This section defines the tests used to verify the system’s compliance with the
Functional Requirements specified in Section~9.1 of the SRS. Each test ensures
that the implemented features meet the expected behavior and fit criteria
described in the Software Requirements Specification. 

The subsections are organized according to the main areas of functionality of
the platform:
\begin{itemize}
    \item Data Filtering and Browsing (FUNC.R.1--R.3)
    \item Behavioral Analysis and Visualization (FUNC.R.4--R.5)
    \item Data Downloading and Accessibility (FUNC.R.6--R.7)
\end{itemize}

\subsection{Tests for Functional Requirements}

This section provides detailed tests for all functional requirements. Each test
includes a test ID, control type, initial conditions, inputs, expected outputs,
test case derivation, and execution description. References to the SRS are
included for traceability. 

\subsubsection{Data Filtering and Browsing}

These tests verify that users can filter, browse, and query data based on
predefined labels, natural language queries, and preset datasets. They validate
the backend (FastAPI + PostgreSQL) integration with the frontend filtering and
NLP modules.

\paragraph{Test 1: Verify Filtering by Predefined Labels}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.1
    \item \textbf{Test ID:} FR1-Filter-Labels
    \item \textbf{Control:} Automatic (API + UI test)
    \item \textbf{Initial State:} The database contains labeled behavioral trials with fields such as \texttt{behavior\_label}, \texttt{trial\_id}, and \texttt{session\_id}.
    \item \textbf{Input:} User selects a filter, e.g., \texttt{behavior\_label = 'checking'}.
    \item \textbf{Output:} Returned dataset contains only trials where \texttt{behavior\_label == 'checking'}; 100\% match accuracy.
    \item \textbf{Test Case Derivation:} Since the fit criterion requires perfect accuracy between filter condition and output, the expected output is all rows meeting the filter condition.
    \item \textbf{How test will be performed:} Use an automated test script (e.g., Postman or PyTest) to send GET requests to \texttt{/api/filter?behavior\_label=checking}. Compare returned data with a query directly run on the database to verify no mismatched rows.
\end{itemize}

\paragraph{Test 2: Verify NLP-Based Query Returns Correct Dataset}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.2
    \item \textbf{Test ID:} FR2-NLP-Query
    \item \textbf{Control:} Manual + Automatic hybrid (human semantic check)
    \item \textbf{Initial State:} NLP model and API are running. Database populated with varied behavioral patterns.
    \item \textbf{Input:} User enters: ``Show me trials with strong checking behavior after 5 injections.''
    \item \textbf{Output:} One dataset is returned that matches this semantic filter.
    \item \textbf{Test Case Derivation:} Based on the SRS fit criterion, one dataset should always be returned for a valid query, containing the subset matching the interpreted condition.
    \item \textbf{How test will be performed:} Send query via UI or API. Validate that the system outputs one dataset (non-empty). A human tester verifies semantic correctness (dataset contextually fits query meaning).
\end{itemize}

\paragraph{Test 3: Verify Predefined Data Categories Available for Browsing}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.3
    \item \textbf{Test ID:} FR3-Predefined-Sets
    \item \textbf{Control:} Manual (UI check)
    \item \textbf{Initial State:} System deployed with at least 3 predefined sets (e.g., ``Exploration Trials'', ``Compulsive Behavior'', ``Control Trials'').
    \item \textbf{Input:} User opens the ``Browse'' interface.
    \item \textbf{Output:} At least 3 predefined datasets appear for selection.
    \item \textbf{Test Case Derivation:} Requirement specifies a minimum of 3 sets available for browsing.
    \item \textbf{How test will be performed:} Manually navigate to the Browse page and confirm at least 3 datasets are displayed and accessible.
\end{itemize}

\subsubsection{Behavioral Analysis and Visualization}

These tests ensure that behavioral categorizations and visual representations are correctly generated for user-selected data subsets.

\paragraph{Test 4: Verify Each Trial Has Behavioral Categorization}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.4
    \item \textbf{Test ID:} FR4-Behavior-Metrics
    \item \textbf{Control:} Automatic (backend validation)
    \item \textbf{Initial State:} Database populated with trial records.
    \item \textbf{Input:} Query any trial dataset.
    \item \textbf{Output:} Each trial entry includes at least one behavioral category field (e.g., ``checking'', ``homebase'', etc.).
    \item \textbf{Test Case Derivation:} Requirement demands every trial have a behavior category, so absence of any \texttt{NULL} category field indicates success.
    \item \textbf{How test will be performed:} Run automated query validation across dataset ensuring no missing categorization values.
\end{itemize}

\paragraph{Test 5: Verify Visuals Are Generated for Each Result Set}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.5
    \item \textbf{Test ID:} FR5-Visualization
    \item \textbf{Control:} Automatic (UI rendering check)
    \item \textbf{Initial State:} User has filtered dataset (non-empty result).
    \item \textbf{Input:} User clicks ``Generate Visualization''.
    \item \textbf{Output:} Visualization component renders trajectory or metric chart corresponding to the dataset.
    \item \textbf{Test Case Derivation:} The fit criterion specifies at least one visual per result set; therefore, successful rendering of a chart or trajectory fulfills this requirement.
    \item \textbf{How test will be performed:} Use UI test automation (e.g., Selenium) to trigger visualization and check that the chart container loads successfully (non-empty DOM element).
\end{itemize}

\subsubsection{Data Downloading and Accessibility}

These tests confirm that datasets and visuals can be downloaded correctly and that the platform maintains global accessibility.

\paragraph{Test 6: Verify Data Download and Integrity}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.6
    \item \textbf{Test ID:} FR6-Download
    \item \textbf{Control:} Automatic
    \item \textbf{Initial State:} Dataset generated and displayed.
    \item \textbf{Input:} User clicks ``Download CSV'' or ``Download Visualization''.
    \item \textbf{Output:} File downloaded successfully; first 100 rows match database query results.
    \item \textbf{Test Case Derivation:} Fit criterion requires verification against the first 100 rows; thus, equality of the first 100 entries validates correctness.
    \item \textbf{How test will be performed:} Automated script downloads dataset and compares first 100 entries with direct database query output.
\end{itemize}

\paragraph{Test 7: Verify Global Accessibility and Performance}
\begin{itemize}
    \item \textbf{Related Requirement:} FUNC.R.7
    \item \textbf{Test ID:} FR7-Accessibility
    \item \textbf{Control:} Automatic (load testing + global simulation)
    \item \textbf{Initial State:} Deployed web system with CDN enabled.
    \item \textbf{Input:} Users (or simulated clients) from 3+ regions (e.g., North America, Europe, Asia) access the site.
    \item \textbf{Output:} Average page load time $<$ 4 seconds across all regions; no functional degradation.
    \item \textbf{Test Case Derivation:} Requirement defines measurable metric (4 seconds); success if timing threshold not exceeded.
    \item \textbf{How test will be performed:} Use tools such as Lighthouse, GTmetrix, or JMeter with geo-distributed endpoints to measure average response time and record metrics.
\end{itemize}


\subsection{Tests for Nonfunctional Requirements}

This section defines the test procedures for verifying the nonfunctional
requirements as defined in Sections~10--17 of the SRS. The tests include
evaluations of appearance, usability, performance, robustness, scalability,
maintainability, and compliance. 

Since nonfunctional requirements often describe qualities rather than discrete
functions, many tests in this section are qualitative or metric-based
evaluations. Several involve user studies, performance measurement, and static
reviews instead of strict pass/fail results.

\subsubsection{Look and Feel Requirements}

These tests ensure that the interface design aligns with the intended audience
(non-technical researchers) and provides a simple, familiar, and intuitive
user experience.

\paragraph{Test 1: Interface Resemblance to Webstore/Boutique}
\begin{itemize}
    \item \textbf{Related Requirement:} APP.R.1
    \item \textbf{Test ID:} NFR1-UI-Appearance
    \item \textbf{Type:} Static, Manual (Usability Review)
    \item \textbf{Initial State:} Functional web interface deployed.
    \item \textbf{Input/Condition:} UI inspected by 3 non-technical evaluators.
    \item \textbf{Output/Result:} 80\% of reviewers confirm the interface resembles a webstore-style interface with clear query “packages.”
    \item \textbf{How test will be performed:} Conduct a design walkthrough where users compare the layout to an e-commerce platform. Collect ratings via short usability survey (Appendix~A).
\end{itemize}

\paragraph{Test 2: Simplicity of Interface Presentation}
\begin{itemize}
    \item \textbf{Related Requirement:} APP.R.2
    \item \textbf{Test ID:} NFR2-UI-Simplicity
    \item \textbf{Type:} Static, Manual (Heuristic Evaluation)
    \item \textbf{Initial State:} All major features visible on homepage.
    \item \textbf{Input/Condition:} Evaluators perform inspection for UI density and feature visibility.
    \item \textbf{Output/Result:} Interface should not exceed 6 visible interactive elements on the main view; evaluators report no intimidation or confusion.
    \item \textbf{How test will be performed:} Use heuristic evaluation checklist for cognitive load and visual complexity; summarize reviewer feedback quantitatively.
\end{itemize}

\paragraph{Test 3: Familiar Filtering Presentation}
\begin{itemize}
    \item \textbf{Related Requirement:} APP.R.3
    \item \textbf{Test ID:} NFR3-UI-Familiarity
    \item \textbf{Type:} Static, Manual
    \item \textbf{Initial State:} Filtering and search panel implemented.
    \item \textbf{Input/Condition:} Users asked to find a data subset using filter controls.
    \item \textbf{Output/Result:} 90\% of participants find the filters intuitive and similar to an online storefront.
    \item \textbf{How test will be performed:} Conduct usability test with 5 participants. Record task completion time and perceived ease of use (Likert scale).
\end{itemize}

\subsubsection{Usability and Humanity Requirements}

These tests measure user learning time, ease of use, and understandability.

\paragraph{Test 4: Ease of Use (Floor and Ceiling)}
\begin{itemize}
    \item \textbf{Related Requirements:} EOU.R.1, EOU.R.2
    \item \textbf{Test ID:} NFR4-EaseOfUse
    \item \textbf{Type:} Dynamic, Manual (User Study)
    \item \textbf{Initial State:} Functional system with packaged queries and NLP interface.
    \item \textbf{Input/Condition:} New users complete a task set (select prepackaged query, perform NLP search, generate visualization).
    \item \textbf{Output/Result:} 100\% task completion; average total completion time $<$ 5 minutes for basic operations.
    \item \textbf{How test will be performed:} Conduct usability test with 5 non-technical participants. Record success rate and completion time per task.
\end{itemize}

\paragraph{Test 5: Learning Curve Assessment}
\begin{itemize}
    \item \textbf{Related Requirement:} LEA.R.1
    \item \textbf{Test ID:} NFR5-LearningCurve
    \item \textbf{Type:} Dynamic, Manual
    \item \textbf{Initial State:} Fully functional platform; user documentation accessible.
    \item \textbf{Input/Condition:} Participants with no prior system experience attempt to complete key workflows.
    \item \textbf{Output/Result:} Average learning curve $<$ 30 minutes; all users can independently perform search and visualization tasks.
    \item \textbf{How test will be performed:} Measure total time for users to reach proficiency (defined as completing 3 tasks unassisted).
\end{itemize}

\paragraph{Test 6: Understandability and Terminology Audit}
\begin{itemize}
    \item \textbf{Related Requirements:} UAP.R.1, UAP.R.2
    \item \textbf{Test ID:} NFR6-Understandability
    \item \textbf{Type:} Static, Manual (Content Review)
    \item \textbf{Initial State:} Interface text finalized.
    \item \textbf{Input/Condition:} Non-technical users read through all on-screen text.
    \item \textbf{Output/Result:} At least 90\% of all interface text rated “understandable” by users.
    \item \textbf{How test will be performed:} Conduct terminology walkthrough; classify all words/phrases as technical or intuitive. Replace technical terms as necessary.
\end{itemize}

\paragraph{Test 7: Accessibility Audit}
\begin{itemize}
    \item \textbf{Related Requirement:} ACC.R.1
    \item \textbf{Test ID:} NFR7-Accessibility
    \item \textbf{Type:} Static + Automated
    \item \textbf{Initial State:} Frontend deployed.
    \item \textbf{Input/Condition:} Run Lighthouse, Axe, or WAVE accessibility audit tools.
    \item \textbf{Output/Result:} Accessibility score $\geq$ 90/100; ARIA labels and alt-text coverage 100\%.
    \item \textbf{How test will be performed:} Automated scan using accessibility tools; manual confirmation of ARIA compliance and alt-text for media.
\end{itemize}

\subsubsection{Performance and Reliability Requirements}

These tests verify latency, throughput, fault-tolerance, and data integrity.

\paragraph{Test 8: Query Speed and Latency Benchmark}
\begin{itemize}
    \item \textbf{Related Requirements:} SAL.R.1, SAL.R.2
    \item \textbf{Test ID:} NFR8-Performance
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} Server and database running under normal load.
    \item \textbf{Input/Condition:} Execute 100 queries of varying sizes up to 5,000 records.
    \item \textbf{Output/Result:} Average query time $<$ 2 seconds; request latency $<$ 100 ms.
    \item \textbf{How test will be performed:} Automated benchmark using JMeter or Locust. Generate graph of response time vs. query size.
\end{itemize}

\paragraph{Test 9: Fault-Tolerance and Logging Verification}
\begin{itemize}
    \item \textbf{Related Requirements:} ROFT.R.1, ROFT.R.2
    \item \textbf{Test ID:} NFR9-Robustness
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} Backend running.
    \item \textbf{Input/Condition:} Submit invalid input payloads and simulate network interruptions.
    \item \textbf{Output/Result:} Errors logged without system crash; user receives appropriate feedback message.
    \item \textbf{How test will be performed:} Automated injection of malformed JSON; verify log entries and system stability.
\end{itemize}

\paragraph{Test 10: Capacity and Scalability Evaluation}
\begin{itemize}
    \item \textbf{Related Requirements:} CAP.R.1, CAP.R.2, SOE.R.2
    \item \textbf{Test ID:} NFR10-Capacity
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} System deployed in cloud environment.
    \item \textbf{Input/Condition:} Simulate 250 concurrent users performing queries.
    \item \textbf{Output/Result:} Throughput $\geq$ 5,000 transactions/hour; performance efficiency $\geq$ 80\%.
    \item \textbf{How test will be performed:} Use load testing tools (Locust, K6) to simulate concurrent access and measure throughput metrics.
\end{itemize}

\subsubsection{Maintainability and Support Requirements}

\paragraph{Test 11: Maintainability Code Review}
\begin{itemize}
    \item \textbf{Related Requirements:} MAI.R.1, MAI.R.2
    \item \textbf{Test ID:} NFR11-Maintainability
    \item \textbf{Type:} Static (Code Walkthrough)
    \item \textbf{Initial State:} Source code and documentation complete.
    \item \textbf{Input/Condition:} Independent developer reviews repository structure and documentation.
    \item \textbf{Output/Result:} Reviewer able to build and extend system within 1 hour using provided documentation.
    \item \textbf{How test will be performed:} Conduct peer code walkthrough; independent participant attempts new dataset integration per SRS instructions.
\end{itemize}

\paragraph{Test 12: User Manual Review}
\begin{itemize}
    \item \textbf{Related Requirement:} SUP.R.1
    \item \textbf{Test ID:} NFR12-Documentation
    \item \textbf{Type:} Static, Manual
    \item \textbf{Initial State:} User manual drafted.
    \item \textbf{Input/Condition:} Review manual completeness and clarity.
    \item \textbf{Output/Result:} 90\% of user study participants rate the manual as “clear” or better.
    \item \textbf{How test will be performed:} Conduct documentation walkthrough with usability test participants; collect feedback ratings.
\end{itemize}

\subsubsection{Security and Compliance Requirements}

\paragraph{Test 13: Data Integrity and Abuse Prevention}
\begin{itemize}
    \item \textbf{Related Requirement:} INT.R.1, IMM.R.1, IMM.R.2
    \item \textbf{Test ID:} NFR13-Security
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} Backend deployed behind HTTPS.
    \item \textbf{Input/Condition:} Attempt SQL injection, code injection, and rate-limit violations.
    \item \textbf{Output/Result:} All attacks blocked; valid error message returned; rate-limiting enforced at 100 requests/minute.
    \item \textbf{How test will be performed:} Perform penetration testing using OWASP ZAP; verify all malicious attempts logged and mitigated.
\end{itemize}

\paragraph{Test 14: Standards and Licensing Compliance}
\begin{itemize}
    \item \textbf{Related Requirements:} LEG.R.1, STA.R.1, STA.R.2
    \item \textbf{Test ID:} NFR14-Compliance
    \item \textbf{Type:} Static, Manual Review
    \item \textbf{Initial State:} Code repository finalized.
    \item \textbf{Input/Condition:} Review dependency list and license headers.
    \item \textbf{Output/Result:} All third-party libraries under compatible licenses; repository includes MIT license and citation requirements.
    \item \textbf{How test will be performed:} Conduct static code inspection; verify license documentation for all dependencies.
\end{itemize}


\subsection{Traceability Between Test Cases and Requirements}

Table~\ref{tab:traceability} provides a traceability matrix showing the correspondence between each Software Requirements Specification (SRS) requirement and the test cases defined in Sections~4.1 and~4.2. Each requirement identifier (FUNC.R.\#, APP.R.\#, etc.) is mapped to one or more test case identifiers that verify its implementation or satisfaction.

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{|l|p{10cm}|}
\caption{Traceability Between Test Cases and Requirements}
\label{tab:traceability} \\
\hline
\textbf{Requirement ID} & \textbf{Associated Test Case(s)} \\
\hline
\endfirsthead
\hline
\textbf{Requirement ID} & \textbf{Associated Test Case(s)} \\
\hline
\endhead
\hline \multicolumn{2}{r}{{Continued on next page}} \\
\endfoot
\hline
\endlastfoot

FUNC.R.1 & ST-FR1.1 Filter Accuracy Test \\
FUNC.R.2 & ST-FR2.1 NLP Query Response Test \\
FUNC.R.3 & ST-FR3.1 Predefined Dataset Browsing Test \\
FUNC.R.4 & ST-FR4.1 Behavioural Categorization and Metrics Test \\
FUNC.R.5 & ST-FR5.1 Visualization Generation Test \\
FUNC.R.6 & ST-FR6.1 Data Download and Export Verification Test \\
FUNC.R.7 & ST-FR7.1 Global Accessibility and Latency Test \\
APP.R.1 & ST-NFR1.1 Interface Familiarity Inspection \\
APP.R.2 & ST-NFR1.2 Visual Simplicity Inspection \\
APP.R.3 & ST-NFR1.3 Filter Interface Familiarity Test \\
EOU.R.1 & ST-NFR2.1 Ease of Use (Low Floor) Usability Study \\
EOU.R.2 & ST-NFR2.2 Ease of Use (Low Ceiling) Usability Study \\
LEA.R.1 & ST-NFR3.1 Learning Curve Evaluation Test \\
LEA.R.2 & ST-NFR3.2 Visualization Guidance Test \\
UAP.R.1 & ST-NFR4.1 Technical Abstraction Inspection \\
UAP.R.2 & ST-NFR4.2 Terminology Familiarity Review \\
ACC.R.1 & ST-NFR5.1 Accessibility Compliance Test \\
SAL.R.1 & ST-NFR6.1 Query Response Time Benchmark \\
SAL.R.2 & ST-NFR6.2 Network Latency Benchmark \\
POA.R.1 & ST-NFR7.1 Data Accuracy Cross-Check \\
POA.R.2 & ST-NFR7.2 Database Consistency Test \\
ROFT.R.1 & ST-NFR8.1 Input Error Logging Test \\
ROFT.R.2 & ST-NFR8.2 Backend Fault Tolerance Test \\
CAP.R.1 & ST-NFR9.1 Concurrent User Load Test \\
CAP.R.2 & ST-NFR9.2 Data Volume Stress Test \\
SOE.R.1 & ST-NFR10.1 Module Integration Test \\
SOE.R.2 & ST-NFR10.2 Performance Under Load Test \\
LON.R.1 & ST-NFR11.1 Reliability and Maintenance Longevity Test \\
LON.R.2 & ST-NFR11.2 Cross-OS Compatibility Test \\
EPE.R.1 & ST-NFR12.1 Desktop Environment Performance Test \\
EPE.R.2 & ST-NFR12.2 Multi-OS Execution Verification Test \\
EPE.R.3 & ST-NFR12.3 Environmental Condition Simulation \\
WE.R.1 & ST-NFR13.1 Browser Compatibility Test \\
IWAS.R.1 & ST-NFR14.1 API Communication Protocol Test \\
PROD.R.1 & ST-NFR15.1 Docker/Kubernetes Deployment Test \\
PROD.R.2 & ST-NFR15.2 Repository Update Verification Test \\
REL.R.1 & ST-NFR16.1 Versioning Convention Test \\
REL.R.2 & ST-NFR16.2 Release Consistency Verification Test \\
MAI.R.1 & ST-NFR17.1 Documentation Maintainability Inspection \\
MAI.R.2 & ST-NFR17.2 Dataset Integration Maintenance Test \\
SUP.R.1 & ST-NFR18.1 User Manual Verification Test \\
SUP.R.2 & ST-NFR18.2 Self-Support Functionality Test \\
ADA.R.1 & ST-NFR19.1 Platform Adaptability Test \\
INT.R.1 & ST-NFR20.1 Data Integrity Protection Test \\
IMM.R.1 & ST-NFR21.1 Security Attack Resistance Test \\
IMM.R.2 & ST-NFR21.2 Rate Limiting and HTTPS Enforcement Test \\
LEG.R.1 & ST-NFR22.1 License Compliance Review \\
STA.R.1 & ST-NFR23.1 FRDR Standards Compliance Test \\
STA.R.2 & ST-NFR23.2 Ethical Use of Data Verification Test \\

\end{longtable}



\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\paragraph{Test 7: Accessibility Audit}
\begin{itemize}
    \item \textbf{Related Requirement:} ACC.R.1
    \item \textbf{Test ID:} NFR7-Accessibility
    \item \textbf{Type:} Static, Manual (Accessibility Review)
    \item \textbf{Initial State:} Fully implemented UI with all interactive elements deployed.
    \item \textbf{Input/Condition:} Users and automated accessibility tools evaluate the system.
    \item \textbf{Output/Result:} WCAG 2.1 AA compliance met; all interactive elements accessible via keyboard; screen readers correctly announce content.
    \item \textbf{How test will be performed:} Use accessibility testing tools (e.g., WAVE, Axe) and manual inspection; verify color contrast, tab navigation, and ARIA attributes.
\end{itemize}

\subsubsection{Performance, Robustness, and Scalability Requirements}

\paragraph{Test 8: Query Performance Benchmark}
\begin{itemize}
    \item \textbf{Related Requirement:} SAL.R.1
    \item \textbf{Test ID:} NFR8-Performance
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} System deployed with representative dataset.
    \item \textbf{Input/Condition:} Run standardized queries on backend.
    \item \textbf{Output/Result:} Average query response time $<$ 2 seconds for typical queries; $<$ 4 seconds for complex queries.
    \item \textbf{How test will be performed:} Use automated scripts and CI tools to record response times; compare against thresholds.
\end{itemize}

\paragraph{Test 9: Fault Tolerance}
\begin{itemize}
    \item \textbf{Related Requirement:} ROFT.R.1
    \item \textbf{Test ID:} NFR9-FaultTolerance
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} System deployed with simulated failure scenarios.
    \item \textbf{Input/Condition:} Induce backend failure, network disruption, or unexpected user input.
    \item \textbf{Output/Result:} System continues operation with minimal disruption; errors logged and no data loss occurs.
    \item \textbf{How test will be performed:} Simulate failures in test environment; monitor system responses and error handling.
\end{itemize}

\paragraph{Test 10: Scalability Testing}
\begin{itemize}
    \item \textbf{Related Requirement:} SOE.R.1
    \item \textbf{Test ID:} NFR10-Scalability
    \item \textbf{Type:} Dynamic, Automatic
    \item \textbf{Initial State:} System deployed in staging with incremental load testing.
    \item \textbf{Input/Condition:} Gradually increase number of simultaneous users and dataset size.
    \item \textbf{Output/Result:} System maintains acceptable performance; response time remains within defined thresholds.
    \item \textbf{How test will be performed:} Use load testing tools (e.g., JMeter, Locust) to simulate increasing load; monitor performance metrics.
\end{itemize}

\paragraph{Test 11: Maintainability and Supportability}
\begin{itemize}
    \item \textbf{Related Requirement:} MAI.R.1, SUP.R.1
    \item \textbf{Test ID:} NFR11-Maintainability
    \item \textbf{Type:} Static, Manual (Code Review)
    \item \textbf{Initial State:} Source code in version control with documentation.
    \item \textbf{Input/Condition:} Review code, documentation, and deployment scripts.
    \item \textbf{Output/Result:} Code conforms to style guides; sufficient comments and documentation for maintenance; modular design observed.
    \item \textbf{How test will be performed:} Conduct peer code reviews and documentation walkthroughs.
\end{itemize}

\paragraph{Test 12: Compliance and Legal Checks}
\begin{itemize}
    \item \textbf{Related Requirement:} LEG.R.1, STA.R.1
    \item \textbf{Test ID:} NFR12-Compliance
    \item \textbf{Type:} Static, Manual
    \item \textbf{Initial State:} System fully deployed; policies and regulations reviewed.
    \item \textbf{Input/Condition:} Evaluate compliance with data protection, licensing, and applicable legal standards.
    \item \textbf{Output/Result:} System meets all regulatory and legal requirements; no unlicensed content used.
    \item \textbf{How test will be performed:} Conduct legal and standards compliance audit; document verification steps.
\end{itemize}


\subsection{Usability Survey Questions?}

\par{The following is a list of straightforward and open ended questions, good for both clear answers and to allow room for discussion, 
all of which will be considered for a usability survey.}

\begin{itemize}
  \item{Were the system's features and functionalities easy to find and intuitively placed?}
  \item{Did the system's functionalities respond quickly to your input?}
  \item{Were you aware of what the system was doing at all times and did you receive feedback after performing an action?}
  \item{Were the search and webstore functionalities easy to use?}
  \item{How well do the search filters satisfy your needs?}
  \item{Are the system's information display and data visualization clear and well organized?}
  \item{What were your least favourite parts of using the system to complete a workflow?}
  \item{On a scale of one to five, how easy was it to learn how to use the system (one meaning impossible and five meaning easy)?}
  \item{On a scale of one to ten, how was your experience using the system's UI (one meaning not user-friendly at all and ten meaning very 
  user-friendly)?}
\end{itemize}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \begin{itemize}
        \item What went well writing this deliverable was our groups collaboration and finishing in a timely mannaer for the most part.
        For example, this past week we had two midterms that took away from our time to complete the deliverable. However, we all managed our time
        and finished the VnV plan in time.
  \end{itemize}
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \begin{itemize}
        \item A pain point we experienced during the deliverable was Leo falling ill before submission and our midterms from the last week. We also have another midterm coming up Wednesday, this 
         made us intredibly nervous for time management, but the group submitted on time 
         by focusing and collaborating with each other. Some would take extra parts and make the process better for others.
  \end{itemize}
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.

  \begin{itemize}

    \item \textbf{Static analysis Approaches: }A very basic approach to testing but often an effective, especially when there is a large team to review each other's code. It will be very useful to review the most relevant static analysis approaches both for this test plan and so that we can generally improve our ability to review each other's code. This can include tools like SonarQube or can just be manual static analysis.

    \item \textbf{JMeter or other Load Testing Tools: }We will need to test the performance of our web application to identify how it performs under increased loads. JMeter allows us to create HTTP requests and measure how the system responds. This will be especially useful in testing the system response under increased user load or a request with a large data result.

    \item \textbf{Selenium or other test automation approaches: }Selenium is a good example of an
automated web application testing tool. This is a tool that we expect to use, given
that we will be creating a web application for the front-end of our system. The use of Selenium or a similar tool allows us to create repeatable and autonomous web application inputs that can be used to effectively test our front-end interface.

    \item \textbf{PyTest or other unit testing approaches: }Although not yet composed, the VnV plan
will eventually contain an extensive set of unit tests which need to be implemented
in our system. Reviewing or learning about a relevant unit test implementation approach
will be crucial in effectively implementing robust unit tests in our project.

  
  \end{itemize}





  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?

  \begin{itemize}

    \item \textbf{All team Members - Static Analysis: }As mentioned above, this is an important skill for general code review as well as our testing plan. As a result, it is important for all team members to review this. Internet tutorials such as a SonarQube walkthrough and reviewing the SFWRENG 3S03 class material are both good approaches for this.

    \item \textbf{Nathan and Leo - JMeter: }Both these team members have experience with load testing and thus it would be most effective for them to refresh their skills in JMeter. We expect the testing implementation to be largely compartmentalized so it would be best for each team member to play into their already existing strengths. Good tools to gain knowledge on this topic would be an internet tutorial such as one from tutorialspoint. Additionally, Nathan has a Kubernetes test-bed with JMeter on his device from an old project which would be useful to refresh knowledge.

    \item \textbf{Jeremy - Selenium: }Jeremy has done reasonably extensive work with Selenium and thus it makes the most sense for him to focus on this aspect of the testing technologies. A refresher from the internet, again from tutorialspoint would be useful. Alternatively, Jeremy can review old projects from his time on Coop to refresh his skills.

    \item \textbf{Tim and Aidan - PyTest: }Much like the other technologies, these team members will be taking this knowledge area because it plays into their already existing strengths. Many online tutorials about this can be found. Again, tutorialspoint is a good option (they seem to have tutorials on everything). Additionally, PyTest was covered in our SWFRENG 3S03 class so that is another good option.

  \end{itemize}
\end{enumerate}

\end{document}
